TACC: Starting up job 8363786
TACC: Setting up parallel environment for MVAPICH2+mpispawn.
TACC: Starting parallel tasks...
Grid Size: 0.5000
Number of Bins: 26*26
Bin Size: 0.02
n = 500, simulation time = 0.110728 seconds
 
TACC: Shutdown complete. Exiting.
TACC: Starting up job 8363786
TACC: Setting up parallel environment for MVAPICH2+mpispawn.
TACC: Starting parallel tasks...
Past MPI Bcast
Past MPI Barrier and about to compute forces
[c557-903.stampede.tacc.utexas.edu:mpi_rank_0][error_sighandler] Caught error: Segmentation fault (signal 11)
[c557-903.stampede.tacc.utexas.edu:mpispawn_0][readline] Unexpected End-Of-File on file descriptor 5. MPI process died?
[c557-903.stampede.tacc.utexas.edu:mpispawn_0][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c557-903.stampede.tacc.utexas.edu:mpispawn_0][child_handler] MPI process (rank: 0, pid: 15100) terminated with signal 11 -> abort job
[c557-903.stampede.tacc.utexas.edu:mpirun_rsh][process_mpispawn_connection] mpispawn_0 from node c557-903 aborted: Error while reading a PMI socket (4)
TACC: MPI job exited with code: 1
 
TACC: Shutdown complete. Exiting.
TACC: Starting up job 8363786
TACC: Setting up parallel environment for MVAPICH2+mpispawn.
TACC: Starting parallel tasks...
Past MPI Bcast
Past MPI Barrier and about to compute forces
Past MPI Bcast
Past MPI Barrier and about to compute forces
[c557-903.stampede.tacc.utexas.edu:mpi_rank_0][error_sighandler] Caught error: Segmentation fault (signal 11)
[c557-904.stampede.tacc.utexas.edu:mpi_rank_1][error_sighandler] Caught error: Segmentation fault (signal 11)
[c557-904.stampede.tacc.utexas.edu:mpispawn_1][readline] Unexpected End-Of-File on file descriptor 5. MPI process died?
[c557-904.stampede.tacc.utexas.edu:mpispawn_1][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c557-904.stampede.tacc.utexas.edu:mpispawn_1][child_handler] MPI process (rank: 1, pid: 7138) terminated with signal 11 -> abort job
[c557-903.stampede.tacc.utexas.edu:mpispawn_0][readline] Unexpected End-Of-File on file descriptor 5. MPI process died?
[c557-903.stampede.tacc.utexas.edu:mpispawn_0][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c557-903.stampede.tacc.utexas.edu:mpispawn_0][child_handler] MPI process (rank: 0, pid: 15416) terminated with signal 13 -> abort job
[c557-903.stampede.tacc.utexas.edu:mpirun_rsh][process_mpispawn_connection] mpispawn_1 from node c557-904 aborted: Error while reading a PMI socket (4)
TACC: MPI job exited with code: 1
 
TACC: Shutdown complete. Exiting.
TACC: Starting up job 8363786
TACC: Setting up parallel environment for MVAPICH2+mpispawn.
TACC: Starting parallel tasks...
Past MPI Bcast
Past MPI Bcast
Past MPI Barrier and about to compute forces
[c557-903.stampede.tacc.utexas.edu:mpi_rank_0][error_sighandler] Caught error: Segmentation fault (signal 11)
Past MPI Bcast
Past MPI Barrier and about to compute forces
Past MPI Bcast
Past MPI Barrier and about to compute forces
[c558-001.stampede.tacc.utexas.edu:mpi_rank_2][error_sighandler] Caught error: Segmentation fault (signal 11)
[c558-002.stampede.tacc.utexas.edu:mpi_rank_3][error_sighandler] Caught error: Segmentation fault (signal 11)
Past MPI Barrier and about to compute forces
[c557-904.stampede.tacc.utexas.edu:mpi_rank_1][error_sighandler] Caught error: Segmentation fault (signal 11)
[c557-904.stampede.tacc.utexas.edu:mpispawn_1][readline] Unexpected End-Of-File on file descriptor 5. MPI process died?
[c557-904.stampede.tacc.utexas.edu:mpispawn_1][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c558-001.stampede.tacc.utexas.edu:mpispawn_2][readline] Unexpected End-Of-File on file descriptor 5. MPI process died?
[c558-001.stampede.tacc.utexas.edu:mpispawn_2][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c558-002.stampede.tacc.utexas.edu:mpispawn_3][readline] Unexpected End-Of-File on file descriptor 5. MPI process died?
[c558-002.stampede.tacc.utexas.edu:mpispawn_3][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c558-001.stampede.tacc.utexas.edu:mpispawn_2][child_handler] MPI process (rank: 2, pid: 115464) terminated with signal 11 -> abort job
[c558-002.stampede.tacc.utexas.edu:mpispawn_3][child_handler] MPI process (rank: 3, pid: 122701) terminated with signal 11 -> abort job
[c557-904.stampede.tacc.utexas.edu:mpispawn_1][child_handler] MPI process (rank: 1, pid: 7253) terminated with signal 11 -> abort job
[c557-903.stampede.tacc.utexas.edu:mpispawn_0][readline] Unexpected End-Of-File on file descriptor 5. MPI process died?
[c557-903.stampede.tacc.utexas.edu:mpispawn_0][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c557-903.stampede.tacc.utexas.edu:mpispawn_0][child_handler] MPI process (rank: 0, pid: 15738) terminated with signal 13 -> abort job
[c557-903.stampede.tacc.utexas.edu:mpirun_rsh][process_mpispawn_connection] mpispawn_2 from node c558-001 aborted: Error while reading a PMI socket (4)
TACC: MPI job exited with code: 1
 
TACC: Shutdown complete. Exiting.
TACC: Starting up job 8363786
TACC: Setting up parallel environment for MVAPICH2+mpispawn.
TACC: Starting parallel tasks...
Past MPI Bcast
Past MPI Bcast
Past MPI Bcast
Past MPI Barrier and about to compute forces
Past MPI Bcast
[c557-903.stampede.tacc.utexas.edu:mpi_rank_0][error_sighandler] Caught error: Segmentation fault (signal 11)
Past MPI Barrier and about to compute forces
Past MPI Bcast
[c558-001.stampede.tacc.utexas.edu:mpi_rank_2][error_sighandler] Caught error: Segmentation fault (signal 11)
Past MPI Bcast
Past MPI Barrier and about to compute forces
Past MPI Bcast
Past MPI Bcast
Past MPI Barrier and about to compute forces
Past MPI Barrier and about to compute forces
Finished computing forces
About to move particlesMoved particles
Past MPI Barrier and about to compute forces
[c558-101.stampede.tacc.utexas.edu:mpi_rank_6][error_sighandler] Caught error: Segmentation fault (signal 11)
Past MPI Barrier and about to compute forces
[c557-904.stampede.tacc.utexas.edu:mpi_rank_1][error_sighandler] Caught error: Segmentation fault (signal 11)
[c558-004.stampede.tacc.utexas.edu:mpi_rank_5][error_sighandler] Caught error: Segmentation fault (signal 11)
[c558-003.stampede.tacc.utexas.edu:mpi_rank_4][error_sighandler] Caught error: Segmentation fault (signal 11)
Past MPI Barrier and about to compute forces
[c558-002.stampede.tacc.utexas.edu:mpi_rank_3][error_sighandler] Caught error: Segmentation fault (signal 11)
[c558-101.stampede.tacc.utexas.edu:mpispawn_6][readline] Unexpected End-Of-File on file descriptor 5. MPI process died?
[c558-101.stampede.tacc.utexas.edu:mpispawn_6][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c558-003.stampede.tacc.utexas.edu:mpispawn_4][readline] Unexpected End-Of-File on file descriptor 5. MPI process died?
[c558-003.stampede.tacc.utexas.edu:mpispawn_4][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c558-001.stampede.tacc.utexas.edu:mpispawn_2][readline] Unexpected End-Of-File on file descriptor 5. MPI process died?
[c558-001.stampede.tacc.utexas.edu:mpispawn_2][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c557-904.stampede.tacc.utexas.edu:mpispawn_1][readline] Unexpected End-Of-File on file descriptor 5. MPI process died?
[c557-904.stampede.tacc.utexas.edu:mpispawn_1][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c558-004.stampede.tacc.utexas.edu:mpispawn_5][readline] Unexpected End-Of-File on file descriptor 5. MPI process died?
[c558-004.stampede.tacc.utexas.edu:mpispawn_5][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c558-002.stampede.tacc.utexas.edu:mpispawn_3][readline] Unexpected End-Of-File on file descriptor 5. MPI process died?
[c558-002.stampede.tacc.utexas.edu:mpispawn_3][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c558-101.stampede.tacc.utexas.edu:mpispawn_6][child_handler] MPI process (rank: 6, pid: 105946) terminated with signal 11 -> abort job
[c558-003.stampede.tacc.utexas.edu:mpispawn_4][child_handler] MPI process (rank: 4, pid: 107229) terminated with signal 11 -> abort job
[c557-904.stampede.tacc.utexas.edu:mpispawn_1][child_handler] MPI process (rank: 1, pid: 7367) terminated with signal 11 -> abort job
[c558-004.stampede.tacc.utexas.edu:mpispawn_5][child_handler] MPI process (rank: 5, pid: 109555) terminated with signal 11 -> abort job
[c558-001.stampede.tacc.utexas.edu:mpispawn_2][child_handler] MPI process (rank: 2, pid: 115579) terminated with signal 11 -> abort job
[c558-002.stampede.tacc.utexas.edu:mpispawn_3][child_handler] MPI process (rank: 3, pid: 122816) terminated with signal 11 -> abort job
[c557-903.stampede.tacc.utexas.edu:mpispawn_0][readline] Unexpected End-Of-File on file descriptor 5. MPI process died?
[c557-903.stampede.tacc.utexas.edu:mpispawn_0][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c557-903.stampede.tacc.utexas.edu:mpispawn_0][child_handler] MPI process (rank: 0, pid: 16068) terminated with signal 11 -> abort job
[c557-903.stampede.tacc.utexas.edu:mpirun_rsh][process_mpispawn_connection] mpispawn_6 from node c558-101 aborted: Error while reading a PMI socket (4)
[c558-102.stampede.tacc.utexas.edu:mpispawn_7][read_size] Unexpected End-Of-File on file descriptor 6. MPI process died?
[c558-102.stampede.tacc.utexas.edu:mpispawn_7][read_size] Unexpected End-Of-File on file descriptor 6. MPI process died?
[c558-102.stampede.tacc.utexas.edu:mpispawn_7][handle_mt_peer] Error while reading PMI socket. MPI process died?
TACC: MPI job exited with code: 1
 
TACC: Shutdown complete. Exiting.
TACC: Starting up job 8363786
TACC: Setting up parallel environment for MVAPICH2+mpispawn.
TACC: Starting parallel tasks...
[c558-102.stampede.tacc.utexas.edu:mpispawn_7][report_error] connect() failed: Connection refused (111)
Past MPI Bcast
Past MPI Bcast
Past MPI Bcast
Past MPI Bcast
Past MPI Bcast
Past MPI Bcast
Past MPI Barrier and about to compute forces
Finished computing forcesPast MPI Barrier and about to compute forces
Past MPI Barrier and about to compute forces
Past MPI Bcast

About to move particles[c558-001.stampede.tacc.utexas.edu:mpi_rank_2][error_sighandler] Caught error: Segmentation fault (signal 11)
Past MPI Barrier and about to compute forces
[c558-201.stampede.tacc.utexas.edu:mpi_rank_10][error_sighandler] Caught error: Segmentation fault (signal 11)
[c558-101.stampede.tacc.utexas.edu:mpi_rank_6][error_sighandler] Caught error: Segmentation fault (signal 11)
Past MPI Bcast
Past MPI Bcast
Past MPI Barrier and about to compute forces
Finished computing forces
About to move particlesMoved particles
Past MPI Barrier and about to compute forces
[c558-301.stampede.tacc.utexas.edu:mpi_rank_14][error_sighandler] Caught error: Segmentation fault (signal 11)
Past MPI Bcast
Past MPI Barrier and about to compute forces
Past MPI Barrier and about to compute forces
Past MPI Bcast
[c558-302.stampede.tacc.utexas.edu:mpi_rank_15][error_sighandler] Caught error: Segmentation fault (signal 11)
Past MPI Barrier and about to compute forces
Past MPI Barrier and about to compute forces
[c558-004.stampede.tacc.utexas.edu:mpi_rank_5][error_sighandler] Caught error: Segmentation fault (signal 11)
Error[c558-103.stampede.tacc.utexas.edu:mpi_rank_8][error_sighandler] Caught error: Segmentation fault (signal 11)
Past MPI Bcast
Past MPI Bcast
Past MPI Bcast
Past MPI Barrier and about to compute forces
Past MPI Barrier and about to compute forces
Past MPI Barrier and about to compute forces
[c557-904.stampede.tacc.utexas.edu:mpi_rank_1][error_sighandler] Caught error: Segmentation fault (signal 11)
[c558-202.stampede.tacc.utexas.edu:mpi_rank_11][error_sighandler] Caught error: Segmentation fault (signal 11)
[c558-102.stampede.tacc.utexas.edu:mpi_rank_7][error_sighandler] Caught error: Segmentation fault (signal 11)
Past MPI Bcast
Past MPI Bcast
Past MPI Barrier and about to compute forces
Finished computing forces
Past MPI Barrier and about to compute forces
Past MPI Barrier and about to compute forces
[c558-204.stampede.tacc.utexas.edu:mpi_rank_13][error_sighandler] Caught error: Segmentation fault (signal 11)
[c558-003.stampede.tacc.utexas.edu:mpi_rank_4][error_sighandler] Caught error: Segmentation fault (signal 11)
[c558-104.stampede.tacc.utexas.edu:mpi_rank_9][error_sighandler] Caught error: Segmentation fault (signal 11)
[c558-002.stampede.tacc.utexas.edu:mpi_rank_3][error_sighandler] Caught error: Segmentation fault (signal 11)
[c558-201.stampede.tacc.utexas.edu:mpispawn_10][readline] Unexpected End-Of-File on file descriptor 5. MPI process died?
[c558-201.stampede.tacc.utexas.edu:mpispawn_10][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c558-301.stampede.tacc.utexas.edu:mpispawn_14][readline] Unexpected End-Of-File on file descriptor 5. MPI process died?
[c558-301.stampede.tacc.utexas.edu:mpispawn_14][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c558-001.stampede.tacc.utexas.edu:mpispawn_2][readline] Unexpected End-Of-File on file descriptor 5. MPI process died?
[c558-001.stampede.tacc.utexas.edu:mpispawn_2][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c558-103.stampede.tacc.utexas.edu:mpispawn_8][readline] Unexpected End-Of-File on file descriptor 5. MPI process died?
[c558-103.stampede.tacc.utexas.edu:mpispawn_8][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c558-101.stampede.tacc.utexas.edu:mpispawn_6][readline] Unexpected End-Of-File on file descriptor 5. MPI process died?
[c558-101.stampede.tacc.utexas.edu:mpispawn_6][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c558-003.stampede.tacc.utexas.edu:mpispawn_4][readline] Unexpected End-Of-File on file descriptor 5. MPI process died?
[c558-003.stampede.tacc.utexas.edu:mpispawn_4][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c558-202.stampede.tacc.utexas.edu:mpispawn_11][readline] Unexpected End-Of-File on file descriptor 5. MPI process died?
[c558-202.stampede.tacc.utexas.edu:mpispawn_11][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c558-104.stampede.tacc.utexas.edu:mpispawn_9][readline] Unexpected End-Of-File on file descriptor 5. MPI process died?
[c558-104.stampede.tacc.utexas.edu:mpispawn_9][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c558-302.stampede.tacc.utexas.edu:mpispawn_15][readline] Unexpected End-Of-File on file descriptor 5. MPI process died?
[c558-302.stampede.tacc.utexas.edu:mpispawn_15][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c558-002.stampede.tacc.utexas.edu:mpispawn_3][readline] Unexpected End-Of-File on file descriptor 5. MPI process died?
[c558-002.stampede.tacc.utexas.edu:mpispawn_3][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c558-204.stampede.tacc.utexas.edu:mpispawn_13][readline] Unexpected End-Of-File on file descriptor 5. MPI process died?
[c558-204.stampede.tacc.utexas.edu:mpispawn_13][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c557-903.stampede.tacc.utexas.edu:mpispawn_0][readline] Unexpected End-Of-File on file descriptor 5. MPI process died?
[c557-903.stampede.tacc.utexas.edu:mpispawn_0][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c558-004.stampede.tacc.utexas.edu:mpispawn_5][readline] Unexpected End-Of-File on file descriptor 5. MPI process died?
[c558-004.stampede.tacc.utexas.edu:mpispawn_5][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c557-904.stampede.tacc.utexas.edu:mpispawn_1][readline] Unexpected End-Of-File on file descriptor 5. MPI process died?
[c557-904.stampede.tacc.utexas.edu:mpispawn_1][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c558-201.stampede.tacc.utexas.edu:mpispawn_10][child_handler] MPI process (rank: 10, pid: 48260) terminated with signal 11 -> abort job
[c558-001.stampede.tacc.utexas.edu:mpispawn_2][child_handler] MPI process (rank: 2, pid: 115693) terminated with signal 11 -> abort job
[c558-301.stampede.tacc.utexas.edu:mpispawn_14][child_handler] MPI process (rank: 14, pid: 81394) terminated with signal 11 -> abort job
[c558-104.stampede.tacc.utexas.edu:mpispawn_9][child_handler] MPI process (rank: 9, pid: 48205) terminated with signal 11 -> abort job
[c558-003.stampede.tacc.utexas.edu:mpispawn_4][child_handler] MPI process (rank: 4, pid: 107344) terminated with signal 11 -> abort job
[c558-103.stampede.tacc.utexas.edu:mpispawn_8][child_handler] MPI process (rank: 8, pid: 7447) terminated with signal 11 -> abort job
[c558-002.stampede.tacc.utexas.edu:mpispawn_3][child_handler] MPI process (rank: 3, pid: 122930) terminated with signal 11 -> abort job
[c558-202.stampede.tacc.utexas.edu:mpispawn_11][child_handler] MPI process (rank: 11, pid: 88481) terminated with signal 11 -> abort job
[c558-101.stampede.tacc.utexas.edu:mpispawn_6][child_handler] MPI process (rank: 6, pid: 106061) terminated with signal 11 -> abort job
[c558-302.stampede.tacc.utexas.edu:mpispawn_15][child_handler] MPI process (rank: 15, pid: 91599) terminated with signal 11 -> abort job
[c558-204.stampede.tacc.utexas.edu:mpispawn_13][child_handler] MPI process (rank: 13, pid: 107055) terminated with signal 11 -> abort job
[c557-903.stampede.tacc.utexas.edu:mpispawn_0][child_handler] MPI process (rank: 0, pid: 16418) exited with status 3
[c558-004.stampede.tacc.utexas.edu:mpispawn_5][child_handler] MPI process (rank: 5, pid: 109669) terminated with signal 11 -> abort job
[c557-904.stampede.tacc.utexas.edu:mpispawn_1][child_handler] MPI process (rank: 1, pid: 7481) terminated with signal 11 -> abort job
[c558-102.stampede.tacc.utexas.edu:mpispawn_7][readline] Unexpected End-Of-File on file descriptor 5. MPI process died?
[c558-102.stampede.tacc.utexas.edu:mpispawn_7][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c558-102.stampede.tacc.utexas.edu:mpispawn_7][child_handler] MPI process (rank: 7, pid: 89406) terminated with signal 11 -> abort job
[c558-203.stampede.tacc.utexas.edu:mpispawn_12][read_size] Unexpected End-Of-File on file descriptor 6. MPI process died?
[c558-203.stampede.tacc.utexas.edu:mpispawn_12][read_size] Unexpected End-Of-File on file descriptor 6. MPI process died?
[c558-203.stampede.tacc.utexas.edu:mpispawn_12][handle_mt_peer] Error while reading PMI socket. MPI process died?
TACC: MPI job exited with code: 1
 
TACC: Shutdown complete. Exiting.
TACC: Starting up job 8363786
TACC: Setting up parallel environment for MVAPICH2+mpispawn.
TACC: Starting parallel tasks...
[c558-203.stampede.tacc.utexas.edu:mpispawn_12][report_error] connect() failed: Connection refused (111)
Past MPI Bcast
Past MPI Barrier and about to compute forces
[c557-903.stampede.tacc.utexas.edu:mpi_rank_0][error_sighandler] Caught error: Segmentation fault (signal 11)
Past MPI Bcast
Past MPI Barrier and about to compute forces
[c557-904.stampede.tacc.utexas.edu:mpi_rank_1][error_sighandler] Caught error: Segmentation fault (signal 11)
[c557-904.stampede.tacc.utexas.edu:mpispawn_1][readline] Unexpected End-Of-File on file descriptor 5. MPI process died?
[c557-904.stampede.tacc.utexas.edu:mpispawn_1][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c557-904.stampede.tacc.utexas.edu:mpispawn_1][child_handler] MPI process (rank: 1, pid: 7595) terminated with signal 11 -> abort job
[c557-903.stampede.tacc.utexas.edu:mpispawn_0][readline] Unexpected End-Of-File on file descriptor 5. MPI process died?
[c557-903.stampede.tacc.utexas.edu:mpispawn_0][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c557-903.stampede.tacc.utexas.edu:mpispawn_0][child_handler] MPI process (rank: 0, pid: 16779) terminated with signal 13 -> abort job
[c557-903.stampede.tacc.utexas.edu:mpirun_rsh][process_mpispawn_connection] mpispawn_1 from node c557-904 aborted: Error while reading a PMI socket (4)
TACC: MPI job exited with code: 1
 
TACC: Shutdown complete. Exiting.
TACC: Starting up job 8363786
TACC: Setting up parallel environment for MVAPICH2+mpispawn.
TACC: Starting parallel tasks...
Past MPI Bcast
Past MPI Bcast
Past MPI Bcast
Past MPI Barrier and about to compute forces
Past MPI Bcast
Finished computing forces
About to move particlesPast MPI Barrier and about to compute forces
Past MPI Barrier and about to compute forces
Finished computing forces
Finished computing forces
About to move particlesPast MPI Barrier and about to compute forces
Finished computing forces
[c557-904.stampede.tacc.utexas.edu:mpi_rank_1][error_sighandler] Caught error: Segmentation fault (signal 11)
[c558-001.stampede.tacc.utexas.edu:mpi_rank_2][error_sighandler] Caught error: Segmentation fault (signal 11)
ErrorError[c558-001.stampede.tacc.utexas.edu:mpispawn_2][readline] Unexpected End-Of-File on file descriptor 5. MPI process died?
[c558-001.stampede.tacc.utexas.edu:mpispawn_2][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c557-903.stampede.tacc.utexas.edu:mpispawn_0][readline] Unexpected End-Of-File on file descriptor 5. MPI process died?
[c557-903.stampede.tacc.utexas.edu:mpispawn_0][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c557-904.stampede.tacc.utexas.edu:mpispawn_1][readline] Unexpected End-Of-File on file descriptor 5. MPI process died?
[c557-904.stampede.tacc.utexas.edu:mpispawn_1][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c558-001.stampede.tacc.utexas.edu:mpispawn_2][child_handler] MPI process (rank: 2, pid: 115807) terminated with signal 11 -> abort job
[c558-002.stampede.tacc.utexas.edu:mpispawn_3][readline] Unexpected End-Of-File on file descriptor 5. MPI process died?
[c558-002.stampede.tacc.utexas.edu:mpispawn_3][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c557-903.stampede.tacc.utexas.edu:mpispawn_0][child_handler] MPI process (rank: 0, pid: 17099) exited with status 3
[c557-904.stampede.tacc.utexas.edu:mpispawn_1][child_handler] MPI process (rank: 1, pid: 7709) terminated with signal 11 -> abort job
[c558-002.stampede.tacc.utexas.edu:mpispawn_3][child_handler] MPI process (rank: 3, pid: 123044) exited with status 3
TACC: MPI job exited with code: 1
 
TACC: Shutdown complete. Exiting.
TACC: Starting up job 8363786
TACC: Setting up parallel environment for MVAPICH2+mpispawn.
TACC: Starting parallel tasks...
Past MPI Bcast
Past MPI Bcast
Past MPI Bcast
Past MPI Bcast
Past MPI Bcast
Past MPI Bcast
Past MPI Bcast
Past MPI Bcast
Past MPI Barrier and about to compute forces
Finished computing forces
About to move particlesPast MPI Barrier and about to compute forces
Finished computing forces
About to move particlesPast MPI Barrier and about to compute forces
Finished computing forces
About to move particlesPast MPI Barrier and about to compute forces
Finished computing forces
About to move particlesPast MPI Barrier and about to compute forces
Finished computing forces
About to move particlesPast MPI Barrier and about to compute forces
Finished computing forces
About to move particlesPast MPI Barrier and about to compute forces
Finished computing forces
About to move particlesMoved particles
Past MPI Barrier and about to compute forces
Finished computing forces
About to move particlesErrorErrorErrorMoved particles
[c558-102.stampede.tacc.utexas.edu:mpi_rank_7][error_sighandler] Caught error: Segmentation fault (signal 11)
Moved particles
Moved particles
[c558-003.stampede.tacc.utexas.edu:mpi_rank_4][error_sighandler] Caught error: Segmentation fault (signal 11)
Moved particles
[c558-003.stampede.tacc.utexas.edu:mpispawn_4][readline] Unexpected End-Of-File on file descriptor 5. MPI process died?
[c558-003.stampede.tacc.utexas.edu:mpispawn_4][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c558-003.stampede.tacc.utexas.edu:mpispawn_4][child_handler] MPI process (rank: 4, pid: 107458) terminated with signal 11 -> abort job
[c558-004.stampede.tacc.utexas.edu:mpispawn_5][readline] Unexpected End-Of-File on file descriptor 5. MPI process died?
[c558-004.stampede.tacc.utexas.edu:mpispawn_5][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c558-102.stampede.tacc.utexas.edu:mpispawn_7][readline] Unexpected End-Of-File on file descriptor 5. MPI process died?
[c558-102.stampede.tacc.utexas.edu:mpispawn_7][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c558-004.stampede.tacc.utexas.edu:mpispawn_5][child_handler] MPI process (rank: 5, pid: 109783) exited with status 3
[c558-102.stampede.tacc.utexas.edu:mpispawn_7][child_handler] MPI process (rank: 7, pid: 89523) terminated with signal 13 -> abort job
[c558-001.stampede.tacc.utexas.edu:mpispawn_2][readline] Unexpected End-Of-File on file descriptor 5. MPI process died?
[c558-001.stampede.tacc.utexas.edu:mpispawn_2][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c558-001.stampede.tacc.utexas.edu:mpispawn_2][child_handler] MPI process (rank: 2, pid: 115921) exited with status 3
[c558-101.stampede.tacc.utexas.edu:mpispawn_6][readline] Unexpected End-Of-File on file descriptor 5. MPI process died?
[c558-101.stampede.tacc.utexas.edu:mpispawn_6][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c558-101.stampede.tacc.utexas.edu:mpispawn_6][child_handler] MPI process (rank: 6, pid: 106175) exited with status 3
TACC: MPI job exited with code: 1
 
TACC: Shutdown complete. Exiting.
TACC: Starting up job 8363786
TACC: Setting up parallel environment for MVAPICH2+mpispawn.
TACC: Starting parallel tasks...
[c558-003.stampede.tacc.utexas.edu:mpispawn_4][report_error] connect() failed: Connection refused (111)
[c557-903.stampede.tacc.utexas.edu:mpispawn_0][read_size] Unexpected End-Of-File on file descriptor 10. MPI process died?
[c557-903.stampede.tacc.utexas.edu:mpispawn_0][read_size] Unexpected End-Of-File on file descriptor 10. MPI process died?
[c557-903.stampede.tacc.utexas.edu:mpispawn_0][handle_mt_peer] Error while reading PMI socket. MPI process died?
[c558-004.stampede.tacc.utexas.edu:mpispawn_5][report_error] connect() failed: Connection refused (111)
[c558-102.stampede.tacc.utexas.edu:mpispawn_7][report_error] connect() failed: Connection refused (111)
[c558-001.stampede.tacc.utexas.edu:mpispawn_2][report_error] connect() failed: Connection refused (111)
[c558-101.stampede.tacc.utexas.edu:mpispawn_6][report_error] connect() failed: Connection refused (111)
[c558-002.stampede.tacc.utexas.edu:mpispawn_3][error_sighandler] Caught error: Segmentation fault (signal 11)
bash: line 1: 123156 Segmentation fault      /bin/env LD_LIBRARY_PATH=/opt/apps/intel15/mvapich2/2.1/lib:/opt/apps/intel15/mvapich2/2.1/lib/shared:/opt/apps/intel/15/composer_xe_2015.2.164/mpirt/lib/intel64:/opt/apps/intel/15/composer_xe_2015.2.164/ipp/lib/intel64:/opt/apps/intel/15/composer_xe_2015.2.164/mkl/lib/intel64:/opt/apps/intel/15/composer_xe_2015.2.164/tbb/lib/intel64:/opt/apps/intel/15/composer_xe_2015.2.164/tbb/lib/intel64/gcc4.4:/opt/apps/intel/15/composer_xe_2015.2.164/compiler/lib/intel64:/opt/apps/xsede/gsi-openssh-7.1p2f/lib64:/opt/apps/xsede/gsi-openssh-7.1p2f/lib:: MPISPAWN_MPIRUN_MPD=0 USE_LINEAR_SSH=1 MPISPAWN_MPIRUN_HOST=c557-903.stampede.tacc.utexas.edu MPISPAWN_MPIRUN_HOSTIP=129.114.90.183 MPIRUN_RSH_LAUNCH=1 MPISPAWN_CHECKIN_PORT=44841 MPISPAWN_MPIRUN_PORT=44841 MPISPAWN_NNODES=8 MPISPAWN_GLOBAL_NPROCS=8 MPISPAWN_MPIRUN_ID=17360 MPISPAWN_ARGC=6 MPDMAN_KVS_TEMPLATE=kvs_518_c557-903.stampede.tacc.utexas.edu_17360 MPISPAWN_LOCAL_NPROCS=1 MPISPAWN_ARGV_0='/work/04661/tg839601/parallelhw2/mpi' MPISPAWN_ARGV_1='-n' MPISPAWN_ARGV_2='4000' MPISPAWN_ARGV_3='-no' MPISPAWN_ARGV_4='-s' MPISPAWN_ARGV_5='mpi_sum.txt' MPISPAWN_ARGC=6 MPISPAWN_GENERIC_ENV_COUNT=138 MPISPAWN_GENERIC_NAME_0=TACC_ENVLEN MPISPAWN_GENERIC_VALUE_0=7314 MPISPAWN_GENERIC_NAME_1=SLURMD_NODENAME MPISPAWN_GENERIC_VALUE_1="c557-903" MPISPAWN_GENERIC_NAME_2=STOCKYARD MPISPAWN_GENERIC_VALUE_2="/work/04661/tg839601" MPISPAWN_GENERIC_NAME_3=SLURM_JOBID MPISPAWN_GENERIC_VALUE_3="8363786" MPISPAWN_GENERIC_NAME_4=TACC_FAMILY_MPI_VERSION MPISPAWN_GENERIC_VALUE_4="2.1" MPISPAWN_GENERIC_NAME_5=_ModuleTable001_ MPISPAWN_GENERIC_VALUE_5="X01vZHVsZVRhYmxlXz17WyJNVHZlcnNpb24iXT0zLFsiY19yZWJ1aWxkVGltZSJdPWZhbHNlLFsiY19zaG9ydFRpbWUiXT1mYWxzZSxkZXB0aFQ9e30sZmFtaWx5PXtbIk1QSSJdPSJtdmFwaWNoMiIsWyJjb21waWxlciJdPSJpbnRlbCIsfSxtVD17VEFDQz17WyJmbiJdPSIvb3B0L21vZHVsZWZpbGVzL1RBQ0MubHVhIixbImZ1bGxOYW1lIl09IlRBQ0MiLFsibG9hZE9yZGVyIl09NCxwcm9wVD17fSxbInN0YXR1cyJdPSJhY3RpdmUiLFsidXNlck5hbWUiXT0iVEFDQyIsfSxpbnRlbD17WyJmbiJdPSIvb3B0L2FwcHMvbW9kdWxlZmlsZXMvaW50ZWwvMTUuMC4yLmx1YSIsWyJmdWxsTmFtZSJdPSJpbnRlbC8xNS4wLjIiLFsibG9hZE9yZGVyIl09MSxwcm9wVD17fSxbInN0YXR1" MPISPAWN_GENERIC_NAME_6=TACC_SYSTEM MPISPAWN_GENERIC_VALUE_6="stampede" MPISPAWN_GENERIC_NAME_7=WORK MPISPAWN_GENERIC_VALUE_7="/work/04661/tg839601" MPISPAWN_GENERIC_NAME_8=ICC_LIB MPISPAWN_GENERIC_VALUE_8="/opt/apps/intel/15/composer_xe_2015.2.164/compiler/lib/intel64" MPISPAWN_GENERIC_NAME_9=SLURM_GTIDS MPISPAWN_GENERIC_VALUE_9="0" MPISPAWN_GENERIC_NAME_10=MIC_LD_LIBRARY_PATH MPISPAWN_GENERIC_VALUE_10="/opt/apps/intel/15/composer_xe_2015.2.164/mkl/lib/mic:/opt/apps/intel/15/composer_xe_2015.2.164/tbb/lib/mic:/opt/intel/mic/coi/device-linux-release/lib:/opt/intel/mic/myo/lib:/opt/apps/intel/15/composer_xe_2015.2.164/compiler/lib/mic" MPISPAWN_GENERIC_NAME_11=SLURM_PRIO_PROCESS MPISPAWN_GENERIC_VALUE_11="0" MPISPAWN_GENERIC_NAME_12=_ModuleTable002_ MPISPAWN_GENERIC_VALUE_12="cyJdPSJhY3RpdmUiLFsidXNlck5hbWUiXT0iaW50ZWwiLH0sbXZhcGljaDI9e1siZm4iXT0iL29wdC9hcHBzL2ludGVsMTUvbW9kdWxlZmlsZXMvbXZhcGljaDIvMi4xIixbImZ1bGxOYW1lIl09Im12YXBpY2gyLzIuMSIsWyJsb2FkT3JkZXIiXT0yLHByb3BUPXt9LFsic3RhdHVzIl09ImFjdGl2ZSIsWyJ1c2VyTmFtZSJdPSJtdmFwaWNoMiIsfSx4YWx0PXtbImZuIl09Ii9vcHQvYXBwcy9tb2R1bGVmaWxlcy94YWx0LzAuNi5sdWEiLFsiZnVsbE5hbWUiXT0ieGFsdC8wLjYiLFsibG9hZE9yZGVyIl09Myxwcm9wVD17fSxbInN0YXR1cyJdPSJhY3RpdmUiLFsidXNlck5hbWUiXT0ieGFsdCIsfSx9LG1wYXRoQT17Ii9vcHQvYXBwcy9pbnRlbDE1L212YXBpY2gyXzJfMS9tb2R1" MPISPAWN_GENERIC_NAME_13=INPUTRC MPISPAWN_GENERIC_VALUE_13="/etc/inputrc" MPISPAWN_GENERIC_NAME_14=I_MPI_OFA_ADAPTER_NAME MPISPAWN_GENERIC_VALUE_14="mlx4_0" MPISPAWN_GENERIC_NAME_15=QTLIB MPISPAWN_GENERIC_VALUE_15="/usr/lib64/qt-3.3/lib" MPISPAWN_GENERIC_NAME_16=SLURM_CPUS_ON_NODE MPISPAWN_GENERIC_VALUE_16="16" MPISPAWN_GENERIC_NAME_17=SLURM_TACC_JOBNAME MPISPAWN_GENERIC_VALUE_17="auto-particle-mpi16" MPISPAWN_GENERIC_NAME_18=SLURM_NODEID MPISPAWN_GENERIC_VALUE_18="0" MPISPAWN_GENERIC_NAME_19=MIC_I_MPI_OFA_ADAPTER_NAME MPISPAWN_GENERIC_VALUE_19="mlx4_0" MPISPAWN_GENERIC_NAME_20=HISTSIZE MPISPAWN_GENERIC_VALUE_20="1000" MPISPAWN_GENERIC_NAME_21=__TRACKER__ MPISPAWN_GENERIC_VALUE_21="1" MPISPAWN_GENERIC_NAME_22=_ModuleTable_Sz_ MPISPAWN_GENERIC_VALUE_22="3" MPISPAWN_GENERIC_NAME_23=SLURM_TOPOLOGY_ADDR MPISPAWN_GENERIC_VALUE_23="c557-903" MPISPAWN_GENERIC_NAME_24=TACC_MKL_INC MPISPAWN_GENERIC_VALUE_24="/opt/apps/intel/15/composer_xe_2015.2.164/mkl/include" MPISPAWN_GENERIC_NAME_25=TACC_XALT_DIR MPISPAWN_GENERIC_VALUE_25="/opt/apps/xalt/0.6/" MPISPAWN_GENERIC_NAME_26=SLURM_SUBMIT_DIR MPISPAWN_GENERIC_VALUE_26="/work/04661/tg839601/parallelhw2" MPISPAWN_GENERIC_NAME_27=HOSTNAME MPISPAWN_GENERIC_VALUE_27="c557-903" MPISPAWN_GENERIC_NAME_28=APPS MPISPAWN_GENERIC_VALUE_28="/opt/apps" MPISPAWN_GENERIC_NAME_29=SLURM_JOB_USER MPISPAWN_GENERIC_VALUE_29="tg839601" MPISPAWN_GENERIC_NAME_30=SHELL MPISPAWN_GENERIC_VALUE_30="/bin/bash" MPISPAWN_GENERIC_NAME_31=TACC_FAMILY_MPI MPISPAWN_GENERIC_VALUE_31="mvapich2" MPISPAWN_GENERIC_NAME_32=TACC_PUBLIC_MACHINE MPISPAWN_GENERIC_VALUE_32="1" MPISPAWN_GENERIC_NAME_33=MV2_USE_OLD_BCAST MPISPAWN_GENERIC_VALUE_33="0" MPISPAWN_GENERIC_NAME_34=SLURM_JOB_NUM_NODES MPISPAWN_GENERIC_VALUE_34="16" MPISPAWN_GENERIC_NAME_35=SLURM_TASK_PID MPISPAWN_GENERIC_VALUE_35="14662" MPISPAWN_GENERIC_NAME_36=TACC_MPI_GETMODE MPISPAWN_GENERIC_VALUE_36="mvapich2_ssh" MPISPAWN_GENERIC_NAME_37=LANG MPISPAWN_GENERIC_VALUE_37="en_US.UTF-8" MPISPAWN_GENERIC_NAME_38=SLURM_LOCALID MPISPAWN_GENERIC_VALUE_38="0" MPISPAWN_GENERIC_NAME_39=SLURM_NPROCS MPISPAWN_GENERIC_VALUE_39="16" MPISPAWN_GENERIC_NAME_40=IBRUN_DEFAULTS MPISPAWN_GENERIC_VALUE_40="/usr/local/bin/ibrun.defaults" MPISPAWN_GENERIC_NAME_41=SLURM_TACC_NNODES_SET MPISPAWN_GENERIC_VALUE_41="1" MPISPAWN_GENERIC_NAME_42=LMOD_SETTARG_CMD MPISPAWN_GENERIC_VALUE_42=":" MPISPAWN_GENERIC_NAME_43=SLURM_JOB_UID MPISPAWN_GENERIC_VALUE_43="839601" MPISPAWN_GENERIC_NAME_44=I_MPI_EXTRA_FILESYSTEM_LIST MPISPAWN_GENERIC_VALUE_44="lustre" MPISPAWN_GENERIC_NAME_45=ibrun_o_option MPISPAWN_GENERIC_VALUE_45="0" MPISPAWN_GENERIC_NAME_46=MIC_ENV_PREFIX MPISPAWN_GENERIC_VALUE_46="MIC" MPISPAWN_GENERIC_NAME_47=LMOD_SYSTEM_DEFAULT_MODULES MPISPAWN_GENERIC_VALUE_47="TACC" MPISPAWN_GENERIC_NAME_48=CPATH MPISPAWN_GENERIC_VALUE_48="/opt/apps/intel/15/composer_xe_2015.2.164/mkl/include:/opt/apps/intel/15/composer_xe_2015.2.164/tbb/include" MPISPAWN_GENERIC_NAME_49=LD_LIBRARY_PATH MPISPAWN_GENERIC_VALUE_49="/opt/apps/intel15/mvapich2/2.1/lib:/opt/apps/intel15/mvapich2/2.1/lib/shared:/opt/apps/intel/15/composer_xe_2015.2.164/mpirt/lib/intel64:/opt/apps/intel/15/composer_xe_2015.2.164/ipp/lib/intel64:/opt/apps/intel/15/composer_xe_2015.2.164/mkl/lib/intel64:/opt/apps/intel/15/composer_xe_2015.2.164/tbb/lib/intel64:/opt/apps/intel/15/composer_xe_2015.2.164/tbb/lib/intel64/gcc4.4:/opt/apps/intel/15/composer_xe_2015.2.164/compiler/lib/intel64:/opt/apps/xsede/gsi-openssh-7.1p2f/lib64:/opt/apps/xsede/gsi-openssh-7.1p2f/lib::" MPISPAWN_GENERIC_NAME_50=PKG_CONFIG_PATH MPISPAWN_GENERIC_VALUE_50="/opt/apps/intel15/mvapich2/2.1/lib/pkgconfig" MPISPAWN_GENERIC_NAME_51=LMOD_PREPEND_BLOCK MPISPAWN_GENERIC_VALUE_51="normal" MPISPAWN_GENERIC_NAME_52=TACC_FAMILY_COMPILER MPISPAWN_GENERIC_VALUE_52="intel" MPISPAWN_GENERIC_NAME_53=SLURM_CLUSTER_NAME MPISPAWN_GENERIC_VALUE_53="stampede" MPISPAWN_GENERIC_NAME_54=TACC_DOMAIN MPISPAWN_GENERIC_VALUE_54="stampede" MPISPAWN_GENERIC_NAME_55=MV2_HOMOGENEOUS_CLUSTER MPISPAWN_GENERIC_VALUE_55="1" MPISPAWN_GENERIC_NAME_56=LMOD_FAMILY_COMPILER MPISPAWN_GENERIC_VALUE_56="intel" MPISPAWN_GENERIC_NAME_57=MV2_USE_UD_HYBRID MPISPAWN_GENERIC_VALUE_57="0" MPISPAWN_GENERIC_NAME_58=I_MPI_FABRICS MPISPAWN_GENERIC_VALUE_58="shm:dapl" MPISPAWN_GENERIC_NAME_59=SLURM_PROCID MPISPAWN_GENERIC_VALUE_59="0" MPISPAWN_GENERIC_NAME_60=GLOBUS_LOCATION MPISPAWN_GENERIC_VALUE_60="/opt/apps/xsede/gsi-openssh-7.1p2f" MPISPAWN_GENERIC_NAME_61=LMOD_FAMILY_COMPILER_VERSION MPISPAWN_GENERIC_VALUE_61="15.0.2" MPISPAWN_GENERIC_NAME_62=LMOD_COLORIZE MPISPAWN_GENERIC_VALUE_62="yes" MPISPAWN_GENERIC_NAME_63=__KNLNAG__ MPISPAWN_GENERIC_VALUE_63="1" MPISPAWN_GENERIC_NAME_64=MV2_DEFAULT_TIME_OUT MPISPAWN_GENERIC_VALUE_64="23" MPISPAWN_GENERIC_NAME_65=MV2_USE_RING_STARTUP MPISPAWN_GENERIC_VALUE_65="0" MPISPAWN_GENERIC_NAME_66=NLSPATH MPISPAWN_GENERIC_VALUE_66="/opt/apps/intel/15/composer_xe_2015.2.164/compiler/lib/intel64/locale/%l_%t/%N:/opt/apps/intel/15/composer_xe_2015.2.164/mkl/lib/intel64/locale/%l_%t/%N" MPISPAWN_GENERIC_NAME_67=TMPDIR MPISPAWN_GENERIC_VALUE_67="/tmp" MPISPAWN_GENERIC_NAME_68=SLURM_JOB_PARTITION MPISPAWN_GENERIC_VALUE_68="development" MPISPAWN_GENERIC_NAME_69=SLURM_JOB_ID MPISPAWN_GENERIC_VALUE_69="8363786" MPISPAWN_GENERIC_NAME_70=LMOD_FAMILY_MPI_VERSION MPISPAWN_GENERIC_VALUE_70="2.1" MPISPAWN_GENERIC_NAME_71=TACC_MKL_DIR MPISPAWN_GENERIC_VALUE_71="/opt/apps/intel/15/composer_xe_2015.2.164/mkl" MPISPAWN_GENERIC_NAME_72=SLURM_CHECKPOINT_IMAGE_DIR MPISPAWN_GENERIC_VALUE_72="/var/slurm/checkpoint" MPISPAWN_GENERIC_NAME_73=LC_ALL MPISPAWN_GENERIC_VALUE_73="en_US.UTF-8" MPISPAWN_GENERIC_NAME_74=LMOD_FAMILY_MPI MPISPAWN_GENERIC_VALUE_74="mvapich2" MPISPAWN_GENERIC_NAME_75=INTEL_PYTHONHOME MPISPAWN_GENERIC_VALUE_75="/opt/apps/intel/15/composer_xe_2015.2.164/debugger/python/intel64" MPISPAWN_GENERIC_NAME_76=SCRATCH MPISPAWN_GENERIC_VALUE_76="/scratch/04661/tg839601" MPISPAWN_GENERIC_NAME_77=randint MPISPAWN_GENERIC_VALUE_77="4" MPISPAWN_GENERIC_NAME_78=LOGNAME MPISPAWN_GENERIC_VALUE_78="tg839601" MPISPAWN_GENERIC_NAME_79=DAPL_ACK_TIMER MPISPAWN_GENERIC_VALUE_79="23" MPISPAWN_GENERIC_NAME_80=LMOD_CMD MPISPAWN_GENERIC_VALUE_80="/opt/apps/lmod/lmod/libexec/lmod" MPISPAWN_GENERIC_NAME_81=NODE_TASKS_PPN_INFO MPISPAWN_GENERIC_VALUE_81="1,0_" MPISPAWN_GENERIC_NAME_82=TACC_FAMILY_COMPILER_VERSION MPISPAWN_GENERIC_VALUE_82="15.0.2" MPISPAWN_GENERIC_NAME_83=PATH MPISPAWN_GENERIC_VALUE_83="/opt/apps/xalt/0.6/bin:/opt/apps/intel15/mvapich2/2.1/bin:/opt/apps/intel/15/composer_xe_2015.2.164/bin/intel64:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/opt/apps/xsede/gsi-openssh-7.1p2f/bin:/usr/X11R6/bin:/opt/ofed/bin:/opt/ofed/sbin:." MPISPAWN_GENERIC_NAME_84=LMOD_PKG MPISPAWN_GENERIC_VALUE_84="/opt/apps/lmod/lmod" MPISPAWN_GENERIC_NAME_85=DAPL_UCM_RETRY MPISPAWN_GENERIC_VALUE_85="10" MPISPAWN_GENERIC_NAME_86=SLURM_NTASKS MPISPAWN_GENERIC_VALUE_86="16" MPISPAWN_GENERIC_NAME_87=LIBRARY_PATH MPISPAWN_GENERIC_VALUE_87="/opt/apps/intel/15/composer_xe_2015.2.164/ipp/lib/intel64:/opt/apps/intel/15/composer_xe_2015.2.164/mkl/lib/intel64:/opt/apps/intel/15/composer_xe_2015.2.164/tbb/lib/intel64:/opt/apps/intel/15/composer_xe_2015.2.164/compiler/lib/intel64" MPISPAWN_GENERIC_NAME_88=TACC_MKL_LIB MPISPAWN_GENERIC_VALUE_88="/opt/apps/intel/15/composer_xe_2015.2.164/mkl/lib/intel64" MPISPAWN_GENERIC_NAME_89=I_MPI_EXTRA_FILESYSTEM MPISPAWN_GENERIC_VALUE_89="enable" MPISPAWN_GENERIC_NAME_90=MV2_USE_HUGEPAGES MPISPAWN_GENERIC_VALUE_90="0" MPISPAWN_GENERIC_NAME_91=MAIL MPISPAWN_GENERIC_VALUE_91="/var/spool/mail/tg839601" MPISPAWN_GENERIC_NAME_92=QTDIR MPISPAWN_GENERIC_VALUE_92="/usr/lib64/qt-3.3" MPISPAWN_GENERIC_NAME_93=DAPL_UCM_REP_TIME MPISPAWN_GENERIC_VALUE_93="8000" MPISPAWN_GENERIC_NAME_94=DAPL_UCM_RTU_TIME MPISPAWN_GENERIC_VALUE_94="4000" MPISPAWN_GENERIC_NAME_95=SLURM_TACC_RUNLIMIT_MINS MPISPAWN_GENERIC_VALUE_95="10" MPISPAWN_GENERIC_NAME_96=TACC_ICC_DIR MPISPAWN_GENERIC_VALUE_96="/opt/apps/intel/15/composer_xe_2015.2.164" MPISPAWN_GENERIC_NAME_97=ARCHIVE MPISPAWN_GENERIC_VALUE_97="/home/04661/tg839601" MPISPAWN_GENERIC_NAME_98=MIC_LIBRARY_PATH MPISPAWN_GENERIC_VALUE_98="/opt/apps/intel/15/composer_xe_2015.2.164/tbb/lib/mic:/opt/apps/intel/15/composer_xe_2015.2.164/mpirt/lib/mic:/opt/apps/intel/15/composer_xe_2015.2.164/compiler/lib/mic" MPISPAWN_GENERIC_NAME_99=_ MPISPAWN_GENERIC_VALUE_99="/usr/local/bin/build_env.pl" MPISPAWN_GENERIC_NAME_100=__BASHRC_SOURCED__ MPISPAWN_GENERIC_VALUE_100="1" MPISPAWN_GENERIC_NAME_101=ARMCI_OPENIB_DEVICE MPISPAWN_GENERIC_VALUE_101="mlx4_0" MPISPAWN_GENERIC_NAME_102=ENVIRONMENT MPISPAWN_GENERIC_VALUE_102="BATCH" MPISPAWN_GENERIC_NAME_103=_ModuleTable003_ MPISPAWN_GENERIC_VALUE_103="bGVmaWxlcyIsIi9vcHQvYXBwcy9pbnRlbDE1L21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL3hzZWRlL21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL21vZHVsZWZpbGVzIiwiL29wdC9tb2R1bGVmaWxlcyIsfSxbInN5c3RlbUJhc2VNUEFUSCJdPSIvb3B0L2FwcHMveHNlZGUvbW9kdWxlZmlsZXM6L29wdC9hcHBzL21vZHVsZWZpbGVzOi9vcHQvbW9kdWxlZmlsZXMiLH0=" MPISPAWN_GENERIC_NAME_104=SLURM_TACC_NODES MPISPAWN_GENERIC_VALUE_104="16" MPISPAWN_GENERIC_NAME_105=MV2_IBA_HCA MPISPAWN_GENERIC_VALUE_105="mlx4_0" MPISPAWN_GENERIC_NAME_106=SINK_LD_LIBRARY_PATH MPISPAWN_GENERIC_VALUE_106="/opt/apps/intel/15/composer_xe_2015.2.164/compiler/lib/mic:/opt/apps/intel/15/composer_xe_2015.2.164/mkl/lib/mic" MPISPAWN_GENERIC_NAME_107=QTINC MPISPAWN_GENERIC_VALUE_107="/usr/lib64/qt-3.3/include" MPISPAWN_GENERIC_NAME_108=CVS_RSH MPISPAWN_GENERIC_VALUE_108="ssh" MPISPAWN_GENERIC_NAME_109=ICC_BIN MPISPAWN_GENERIC_VALUE_109="/opt/apps/intel/15/composer_xe_2015.2.164/bin/intel64" MPISPAWN_GENERIC_NAME_110=USER MPISPAWN_GENERIC_VALUE_110="tg839601" MPISPAWN_GENERIC_NAME_111=MIC_I_MPI_DAPL_PROVIDER MPISPAWN_GENERIC_VALUE_111="ofa-v2-mlx4_0-1u" MPISPAWN_GENERIC_NAME_112=SLURM_JOB_QOS MPISPAWN_GENERIC_VALUE_112="normal" MPISPAWN_GENERIC_NAME_113=G_BROKEN_FILENAMES MPISPAWN_GENERIC_VALUE_113="1" MPISPAWN_GENERIC_NAME_114=SLURM_TACC_CORES MPISPAWN_GENERIC_VALUE_114="16" MPISPAWN_GENERIC_NAME_115=LMOD_DIR MPISPAWN_GENERIC_VALUE_115="/opt/apps/lmod/lmod/libexec" MPISPAWN_GENERIC_NAME_116=OMP_NUM_THREADS MPISPAWN_GENERIC_VALUE_116="1" MPISPAWN_GENERIC_NAME_117=__PERSONAL_PATH__ MPISPAWN_GENERIC_VALUE_117="1" MPISPAWN_GENERIC_NAME_118=SLURM_JOB_NAME MPISPAWN_GENERIC_VALUE_118="auto-particle-mpi16" MPISPAWN_GENERIC_NAME_119=SLURM_SUBMIT_HOST MPISPAWN_GENERIC_VALUE_119="login3.stampede.tacc.utexas.edu" MPISPAWN_GENERIC_NAME_120=SLURM_QUEUE MPISPAWN_GENERIC_VALUE_120="development" MPISPAWN_GENERIC_NAME_121=SLURM_TACC_ACCOUNT MPISPAWN_GENERIC_VALUE_121="TG-CCR140008" MPISPAWN_GENERIC_NAME_122=TBBROOT MPISPAWN_GENERIC_VALUE_122="/opt/apps/intel/15/composer_xe_2015.2.164/tbb" MPISPAWN_GENERIC_NAME_123=MPICH_HOME MPISPAWN_GENERIC_VALUE_123="/opt/apps/intel15/mvapich2/2.1" MPISPAWN_GENERIC_NAME_124=SLURM_COMMAND MPISPAWN_GENERIC_VALUE_124="sbatch" MPISPAWN_GENERIC_NAME_125=I_MPI_DAPL_PROVIDER MPISPAWN_GENERIC_VALUE_125="ofa-v2-mlx4_0-1u" MPISPAWN_GENERIC_NAME_126=SLURM_JOB_NODELIST MPISPAWN_GENERIC_VALUE_126="c557-[903-904],c558-[001-004,101-104,201-204,301-302]" MPISPAWN_GENERIC_NAME_127=SLURM_JOB_ACCOUNT MPISPAWN_GENERIC_VALUE_127="TG-CCR140008" MPISPAWN_GENERIC_NAME_128=SLURM_NNODES MPISPAWN_GENERIC_VALUE_128="16" MPISPAWN_GENERIC_NAME_129=MKLROOT MPISPAWN_GENERIC_VALUE_129="/opt/apps/intel/15/composer_xe_2015.2.164/mkl" MPISPAWN_GENERIC_NAME_130=SLURM_TACC_NCORES_SET MPISPAWN_GENERIC_VALUE_130="1" MPISPAWN_GENERIC_NAME_131=__Init_Default_Modules MPISPAWN_GENERIC_VALUE_131="1" MPISPAWN_GENERIC_NAME_132=SLURM_TOPOLOGY_ADDR_PATTERN MPISPAWN_GENERIC_VALUE_132="node" MPISPAWN_GENERIC_NAME_133=BASH_ENV MPISPAWN_GENERIC_VALUE_133="/etc/tacc/tacc_functions" MPISPAWN_GENERIC_NAME_134=TACC_XALT_BIN MPISPAWN_GENERIC_VALUE_134="/opt/apps/xalt/0.6/bin" MPISPAWN_GENERIC_NAME_135=X509_USER_PROXY MPISPAWN_GENERIC_VALUE_135="/tmp/x509up_p34218.fileHy64Io.1" MPISPAWN_GENERIC_NAME_136=LMOD_FULL_SETTARG_SUPPORT MPISPAWN_GENERIC_VALUE_136="no" MPISPAWN_GENERIC_NAME_137=LMOD_VERSION MPISPAWN_GENERIC_VALUE_137="7.3.18" MPISPAWN_ID=3 MPISPAWN_WORKING_DIR=/work/04661/tg839601/parallelhw2 MPISPAWN_MPIRUN_RANK_0=3 /opt/apps/intel15/mvapich2/2.1/bin/mpispawn 0
[c557-904.stampede.tacc.utexas.edu:mpispawn_1][error_sighandler] Caught error: Segmentation fault (signal 11)
bash: line 1:  7821 Segmentation fault      /bin/env LD_LIBRARY_PATH=/opt/apps/intel15/mvapich2/2.1/lib:/opt/apps/intel15/mvapich2/2.1/lib/shared:/opt/apps/intel/15/composer_xe_2015.2.164/mpirt/lib/intel64:/opt/apps/intel/15/composer_xe_2015.2.164/ipp/lib/intel64:/opt/apps/intel/15/composer_xe_2015.2.164/mkl/lib/intel64:/opt/apps/intel/15/composer_xe_2015.2.164/tbb/lib/intel64:/opt/apps/intel/15/composer_xe_2015.2.164/tbb/lib/intel64/gcc4.4:/opt/apps/intel/15/composer_xe_2015.2.164/compiler/lib/intel64:/opt/apps/xsede/gsi-openssh-7.1p2f/lib64:/opt/apps/xsede/gsi-openssh-7.1p2f/lib:: MPISPAWN_MPIRUN_MPD=0 USE_LINEAR_SSH=1 MPISPAWN_MPIRUN_HOST=c557-903.stampede.tacc.utexas.edu MPISPAWN_MPIRUN_HOSTIP=129.114.90.183 MPIRUN_RSH_LAUNCH=1 MPISPAWN_CHECKIN_PORT=44841 MPISPAWN_MPIRUN_PORT=44841 MPISPAWN_NNODES=8 MPISPAWN_GLOBAL_NPROCS=8 MPISPAWN_MPIRUN_ID=17360 MPISPAWN_ARGC=6 MPDMAN_KVS_TEMPLATE=kvs_518_c557-903.stampede.tacc.utexas.edu_17360 MPISPAWN_LOCAL_NPROCS=1 MPISPAWN_ARGV_0='/work/04661/tg839601/parallelhw2/mpi' MPISPAWN_ARGV_1='-n' MPISPAWN_ARGV_2='4000' MPISPAWN_ARGV_3='-no' MPISPAWN_ARGV_4='-s' MPISPAWN_ARGV_5='mpi_sum.txt' MPISPAWN_ARGC=6 MPISPAWN_GENERIC_ENV_COUNT=138 MPISPAWN_GENERIC_NAME_0=TACC_ENVLEN MPISPAWN_GENERIC_VALUE_0=7314 MPISPAWN_GENERIC_NAME_1=SLURMD_NODENAME MPISPAWN_GENERIC_VALUE_1="c557-903" MPISPAWN_GENERIC_NAME_2=STOCKYARD MPISPAWN_GENERIC_VALUE_2="/work/04661/tg839601" MPISPAWN_GENERIC_NAME_3=SLURM_JOBID MPISPAWN_GENERIC_VALUE_3="8363786" MPISPAWN_GENERIC_NAME_4=TACC_FAMILY_MPI_VERSION MPISPAWN_GENERIC_VALUE_4="2.1" MPISPAWN_GENERIC_NAME_5=_ModuleTable001_ MPISPAWN_GENERIC_VALUE_5="X01vZHVsZVRhYmxlXz17WyJNVHZlcnNpb24iXT0zLFsiY19yZWJ1aWxkVGltZSJdPWZhbHNlLFsiY19zaG9ydFRpbWUiXT1mYWxzZSxkZXB0aFQ9e30sZmFtaWx5PXtbIk1QSSJdPSJtdmFwaWNoMiIsWyJjb21waWxlciJdPSJpbnRlbCIsfSxtVD17VEFDQz17WyJmbiJdPSIvb3B0L21vZHVsZWZpbGVzL1RBQ0MubHVhIixbImZ1bGxOYW1lIl09IlRBQ0MiLFsibG9hZE9yZGVyIl09NCxwcm9wVD17fSxbInN0YXR1cyJdPSJhY3RpdmUiLFsidXNlck5hbWUiXT0iVEFDQyIsfSxpbnRlbD17WyJmbiJdPSIvb3B0L2FwcHMvbW9kdWxlZmlsZXMvaW50ZWwvMTUuMC4yLmx1YSIsWyJmdWxsTmFtZSJdPSJpbnRlbC8xNS4wLjIiLFsibG9hZE9yZGVyIl09MSxwcm9wVD17fSxbInN0YXR1" MPISPAWN_GENERIC_NAME_6=TACC_SYSTEM MPISPAWN_GENERIC_VALUE_6="stampede" MPISPAWN_GENERIC_NAME_7=WORK MPISPAWN_GENERIC_VALUE_7="/work/04661/tg839601" MPISPAWN_GENERIC_NAME_8=ICC_LIB MPISPAWN_GENERIC_VALUE_8="/opt/apps/intel/15/composer_xe_2015.2.164/compiler/lib/intel64" MPISPAWN_GENERIC_NAME_9=SLURM_GTIDS MPISPAWN_GENERIC_VALUE_9="0" MPISPAWN_GENERIC_NAME_10=MIC_LD_LIBRARY_PATH MPISPAWN_GENERIC_VALUE_10="/opt/apps/intel/15/composer_xe_2015.2.164/mkl/lib/mic:/opt/apps/intel/15/composer_xe_2015.2.164/tbb/lib/mic:/opt/intel/mic/coi/device-linux-release/lib:/opt/intel/mic/myo/lib:/opt/apps/intel/15/composer_xe_2015.2.164/compiler/lib/mic" MPISPAWN_GENERIC_NAME_11=SLURM_PRIO_PROCESS MPISPAWN_GENERIC_VALUE_11="0" MPISPAWN_GENERIC_NAME_12=_ModuleTable002_ MPISPAWN_GENERIC_VALUE_12="cyJdPSJhY3RpdmUiLFsidXNlck5hbWUiXT0iaW50ZWwiLH0sbXZhcGljaDI9e1siZm4iXT0iL29wdC9hcHBzL2ludGVsMTUvbW9kdWxlZmlsZXMvbXZhcGljaDIvMi4xIixbImZ1bGxOYW1lIl09Im12YXBpY2gyLzIuMSIsWyJsb2FkT3JkZXIiXT0yLHByb3BUPXt9LFsic3RhdHVzIl09ImFjdGl2ZSIsWyJ1c2VyTmFtZSJdPSJtdmFwaWNoMiIsfSx4YWx0PXtbImZuIl09Ii9vcHQvYXBwcy9tb2R1bGVmaWxlcy94YWx0LzAuNi5sdWEiLFsiZnVsbE5hbWUiXT0ieGFsdC8wLjYiLFsibG9hZE9yZGVyIl09Myxwcm9wVD17fSxbInN0YXR1cyJdPSJhY3RpdmUiLFsidXNlck5hbWUiXT0ieGFsdCIsfSx9LG1wYXRoQT17Ii9vcHQvYXBwcy9pbnRlbDE1L212YXBpY2gyXzJfMS9tb2R1" MPISPAWN_GENERIC_NAME_13=INPUTRC MPISPAWN_GENERIC_VALUE_13="/etc/inputrc" MPISPAWN_GENERIC_NAME_14=I_MPI_OFA_ADAPTER_NAME MPISPAWN_GENERIC_VALUE_14="mlx4_0" MPISPAWN_GENERIC_NAME_15=QTLIB MPISPAWN_GENERIC_VALUE_15="/usr/lib64/qt-3.3/lib" MPISPAWN_GENERIC_NAME_16=SLURM_CPUS_ON_NODE MPISPAWN_GENERIC_VALUE_16="16" MPISPAWN_GENERIC_NAME_17=SLURM_TACC_JOBNAME MPISPAWN_GENERIC_VALUE_17="auto-particle-mpi16" MPISPAWN_GENERIC_NAME_18=SLURM_NODEID MPISPAWN_GENERIC_VALUE_18="0" MPISPAWN_GENERIC_NAME_19=MIC_I_MPI_OFA_ADAPTER_NAME MPISPAWN_GENERIC_VALUE_19="mlx4_0" MPISPAWN_GENERIC_NAME_20=HISTSIZE MPISPAWN_GENERIC_VALUE_20="1000" MPISPAWN_GENERIC_NAME_21=__TRACKER__ MPISPAWN_GENERIC_VALUE_21="1" MPISPAWN_GENERIC_NAME_22=_ModuleTable_Sz_ MPISPAWN_GENERIC_VALUE_22="3" MPISPAWN_GENERIC_NAME_23=SLURM_TOPOLOGY_ADDR MPISPAWN_GENERIC_VALUE_23="c557-903" MPISPAWN_GENERIC_NAME_24=TACC_MKL_INC MPISPAWN_GENERIC_VALUE_24="/opt/apps/intel/15/composer_xe_2015.2.164/mkl/include" MPISPAWN_GENERIC_NAME_25=TACC_XALT_DIR MPISPAWN_GENERIC_VALUE_25="/opt/apps/xalt/0.6/" MPISPAWN_GENERIC_NAME_26=SLURM_SUBMIT_DIR MPISPAWN_GENERIC_VALUE_26="/work/04661/tg839601/parallelhw2" MPISPAWN_GENERIC_NAME_27=HOSTNAME MPISPAWN_GENERIC_VALUE_27="c557-903" MPISPAWN_GENERIC_NAME_28=APPS MPISPAWN_GENERIC_VALUE_28="/opt/apps" MPISPAWN_GENERIC_NAME_29=SLURM_JOB_USER MPISPAWN_GENERIC_VALUE_29="tg839601" MPISPAWN_GENERIC_NAME_30=SHELL MPISPAWN_GENERIC_VALUE_30="/bin/bash" MPISPAWN_GENERIC_NAME_31=TACC_FAMILY_MPI MPISPAWN_GENERIC_VALUE_31="mvapich2" MPISPAWN_GENERIC_NAME_32=TACC_PUBLIC_MACHINE MPISPAWN_GENERIC_VALUE_32="1" MPISPAWN_GENERIC_NAME_33=MV2_USE_OLD_BCAST MPISPAWN_GENERIC_VALUE_33="0" MPISPAWN_GENERIC_NAME_34=SLURM_JOB_NUM_NODES MPISPAWN_GENERIC_VALUE_34="16" MPISPAWN_GENERIC_NAME_35=SLURM_TASK_PID MPISPAWN_GENERIC_VALUE_35="14662" MPISPAWN_GENERIC_NAME_36=TACC_MPI_GETMODE MPISPAWN_GENERIC_VALUE_36="mvapich2_ssh" MPISPAWN_GENERIC_NAME_37=LANG MPISPAWN_GENERIC_VALUE_37="en_US.UTF-8" MPISPAWN_GENERIC_NAME_38=SLURM_LOCALID MPISPAWN_GENERIC_VALUE_38="0" MPISPAWN_GENERIC_NAME_39=SLURM_NPROCS MPISPAWN_GENERIC_VALUE_39="16" MPISPAWN_GENERIC_NAME_40=IBRUN_DEFAULTS MPISPAWN_GENERIC_VALUE_40="/usr/local/bin/ibrun.defaults" MPISPAWN_GENERIC_NAME_41=SLURM_TACC_NNODES_SET MPISPAWN_GENERIC_VALUE_41="1" MPISPAWN_GENERIC_NAME_42=LMOD_SETTARG_CMD MPISPAWN_GENERIC_VALUE_42=":" MPISPAWN_GENERIC_NAME_43=SLURM_JOB_UID MPISPAWN_GENERIC_VALUE_43="839601" MPISPAWN_GENERIC_NAME_44=I_MPI_EXTRA_FILESYSTEM_LIST MPISPAWN_GENERIC_VALUE_44="lustre" MPISPAWN_GENERIC_NAME_45=ibrun_o_option MPISPAWN_GENERIC_VALUE_45="0" MPISPAWN_GENERIC_NAME_46=MIC_ENV_PREFIX MPISPAWN_GENERIC_VALUE_46="MIC" MPISPAWN_GENERIC_NAME_47=LMOD_SYSTEM_DEFAULT_MODULES MPISPAWN_GENERIC_VALUE_47="TACC" MPISPAWN_GENERIC_NAME_48=CPATH MPISPAWN_GENERIC_VALUE_48="/opt/apps/intel/15/composer_xe_2015.2.164/mkl/include:/opt/apps/intel/15/composer_xe_2015.2.164/tbb/include" MPISPAWN_GENERIC_NAME_49=LD_LIBRARY_PATH MPISPAWN_GENERIC_VALUE_49="/opt/apps/intel15/mvapich2/2.1/lib:/opt/apps/intel15/mvapich2/2.1/lib/shared:/opt/apps/intel/15/composer_xe_2015.2.164/mpirt/lib/intel64:/opt/apps/intel/15/composer_xe_2015.2.164/ipp/lib/intel64:/opt/apps/intel/15/composer_xe_2015.2.164/mkl/lib/intel64:/opt/apps/intel/15/composer_xe_2015.2.164/tbb/lib/intel64:/opt/apps/intel/15/composer_xe_2015.2.164/tbb/lib/intel64/gcc4.4:/opt/apps/intel/15/composer_xe_2015.2.164/compiler/lib/intel64:/opt/apps/xsede/gsi-openssh-7.1p2f/lib64:/opt/apps/xsede/gsi-openssh-7.1p2f/lib::" MPISPAWN_GENERIC_NAME_50=PKG_CONFIG_PATH MPISPAWN_GENERIC_VALUE_50="/opt/apps/intel15/mvapich2/2.1/lib/pkgconfig" MPISPAWN_GENERIC_NAME_51=LMOD_PREPEND_BLOCK MPISPAWN_GENERIC_VALUE_51="normal" MPISPAWN_GENERIC_NAME_52=TACC_FAMILY_COMPILER MPISPAWN_GENERIC_VALUE_52="intel" MPISPAWN_GENERIC_NAME_53=SLURM_CLUSTER_NAME MPISPAWN_GENERIC_VALUE_53="stampede" MPISPAWN_GENERIC_NAME_54=TACC_DOMAIN MPISPAWN_GENERIC_VALUE_54="stampede" MPISPAWN_GENERIC_NAME_55=MV2_HOMOGENEOUS_CLUSTER MPISPAWN_GENERIC_VALUE_55="1" MPISPAWN_GENERIC_NAME_56=LMOD_FAMILY_COMPILER MPISPAWN_GENERIC_VALUE_56="intel" MPISPAWN_GENERIC_NAME_57=MV2_USE_UD_HYBRID MPISPAWN_GENERIC_VALUE_57="0" MPISPAWN_GENERIC_NAME_58=I_MPI_FABRICS MPISPAWN_GENERIC_VALUE_58="shm:dapl" MPISPAWN_GENERIC_NAME_59=SLURM_PROCID MPISPAWN_GENERIC_VALUE_59="0" MPISPAWN_GENERIC_NAME_60=GLOBUS_LOCATION MPISPAWN_GENERIC_VALUE_60="/opt/apps/xsede/gsi-openssh-7.1p2f" MPISPAWN_GENERIC_NAME_61=LMOD_FAMILY_COMPILER_VERSION MPISPAWN_GENERIC_VALUE_61="15.0.2" MPISPAWN_GENERIC_NAME_62=LMOD_COLORIZE MPISPAWN_GENERIC_VALUE_62="yes" MPISPAWN_GENERIC_NAME_63=__KNLNAG__ MPISPAWN_GENERIC_VALUE_63="1" MPISPAWN_GENERIC_NAME_64=MV2_DEFAULT_TIME_OUT MPISPAWN_GENERIC_VALUE_64="23" MPISPAWN_GENERIC_NAME_65=MV2_USE_RING_STARTUP MPISPAWN_GENERIC_VALUE_65="0" MPISPAWN_GENERIC_NAME_66=NLSPATH MPISPAWN_GENERIC_VALUE_66="/opt/apps/intel/15/composer_xe_2015.2.164/compiler/lib/intel64/locale/%l_%t/%N:/opt/apps/intel/15/composer_xe_2015.2.164/mkl/lib/intel64/locale/%l_%t/%N" MPISPAWN_GENERIC_NAME_67=TMPDIR MPISPAWN_GENERIC_VALUE_67="/tmp" MPISPAWN_GENERIC_NAME_68=SLURM_JOB_PARTITION MPISPAWN_GENERIC_VALUE_68="development" MPISPAWN_GENERIC_NAME_69=SLURM_JOB_ID MPISPAWN_GENERIC_VALUE_69="8363786" MPISPAWN_GENERIC_NAME_70=LMOD_FAMILY_MPI_VERSION MPISPAWN_GENERIC_VALUE_70="2.1" MPISPAWN_GENERIC_NAME_71=TACC_MKL_DIR MPISPAWN_GENERIC_VALUE_71="/opt/apps/intel/15/composer_xe_2015.2.164/mkl" MPISPAWN_GENERIC_NAME_72=SLURM_CHECKPOINT_IMAGE_DIR MPISPAWN_GENERIC_VALUE_72="/var/slurm/checkpoint" MPISPAWN_GENERIC_NAME_73=LC_ALL MPISPAWN_GENERIC_VALUE_73="en_US.UTF-8" MPISPAWN_GENERIC_NAME_74=LMOD_FAMILY_MPI MPISPAWN_GENERIC_VALUE_74="mvapich2" MPISPAWN_GENERIC_NAME_75=INTEL_PYTHONHOME MPISPAWN_GENERIC_VALUE_75="/opt/apps/intel/15/composer_xe_2015.2.164/debugger/python/intel64" MPISPAWN_GENERIC_NAME_76=SCRATCH MPISPAWN_GENERIC_VALUE_76="/scratch/04661/tg839601" MPISPAWN_GENERIC_NAME_77=randint MPISPAWN_GENERIC_VALUE_77="4" MPISPAWN_GENERIC_NAME_78=LOGNAME MPISPAWN_GENERIC_VALUE_78="tg839601" MPISPAWN_GENERIC_NAME_79=DAPL_ACK_TIMER MPISPAWN_GENERIC_VALUE_79="23" MPISPAWN_GENERIC_NAME_80=LMOD_CMD MPISPAWN_GENERIC_VALUE_80="/opt/apps/lmod/lmod/libexec/lmod" MPISPAWN_GENERIC_NAME_81=NODE_TASKS_PPN_INFO MPISPAWN_GENERIC_VALUE_81="1,0_" MPISPAWN_GENERIC_NAME_82=TACC_FAMILY_COMPILER_VERSION MPISPAWN_GENERIC_VALUE_82="15.0.2" MPISPAWN_GENERIC_NAME_83=PATH MPISPAWN_GENERIC_VALUE_83="/opt/apps/xalt/0.6/bin:/opt/apps/intel15/mvapich2/2.1/bin:/opt/apps/intel/15/composer_xe_2015.2.164/bin/intel64:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/opt/apps/xsede/gsi-openssh-7.1p2f/bin:/usr/X11R6/bin:/opt/ofed/bin:/opt/ofed/sbin:." MPISPAWN_GENERIC_NAME_84=LMOD_PKG MPISPAWN_GENERIC_VALUE_84="/opt/apps/lmod/lmod" MPISPAWN_GENERIC_NAME_85=DAPL_UCM_RETRY MPISPAWN_GENERIC_VALUE_85="10" MPISPAWN_GENERIC_NAME_86=SLURM_NTASKS MPISPAWN_GENERIC_VALUE_86="16" MPISPAWN_GENERIC_NAME_87=LIBRARY_PATH MPISPAWN_GENERIC_VALUE_87="/opt/apps/intel/15/composer_xe_2015.2.164/ipp/lib/intel64:/opt/apps/intel/15/composer_xe_2015.2.164/mkl/lib/intel64:/opt/apps/intel/15/composer_xe_2015.2.164/tbb/lib/intel64:/opt/apps/intel/15/composer_xe_2015.2.164/compiler/lib/intel64" MPISPAWN_GENERIC_NAME_88=TACC_MKL_LIB MPISPAWN_GENERIC_VALUE_88="/opt/apps/intel/15/composer_xe_2015.2.164/mkl/lib/intel64" MPISPAWN_GENERIC_NAME_89=I_MPI_EXTRA_FILESYSTEM MPISPAWN_GENERIC_VALUE_89="enable" MPISPAWN_GENERIC_NAME_90=MV2_USE_HUGEPAGES MPISPAWN_GENERIC_VALUE_90="0" MPISPAWN_GENERIC_NAME_91=MAIL MPISPAWN_GENERIC_VALUE_91="/var/spool/mail/tg839601" MPISPAWN_GENERIC_NAME_92=QTDIR MPISPAWN_GENERIC_VALUE_92="/usr/lib64/qt-3.3" MPISPAWN_GENERIC_NAME_93=DAPL_UCM_REP_TIME MPISPAWN_GENERIC_VALUE_93="8000" MPISPAWN_GENERIC_NAME_94=DAPL_UCM_RTU_TIME MPISPAWN_GENERIC_VALUE_94="4000" MPISPAWN_GENERIC_NAME_95=SLURM_TACC_RUNLIMIT_MINS MPISPAWN_GENERIC_VALUE_95="10" MPISPAWN_GENERIC_NAME_96=TACC_ICC_DIR MPISPAWN_GENERIC_VALUE_96="/opt/apps/intel/15/composer_xe_2015.2.164" MPISPAWN_GENERIC_NAME_97=ARCHIVE MPISPAWN_GENERIC_VALUE_97="/home/04661/tg839601" MPISPAWN_GENERIC_NAME_98=MIC_LIBRARY_PATH MPISPAWN_GENERIC_VALUE_98="/opt/apps/intel/15/composer_xe_2015.2.164/tbb/lib/mic:/opt/apps/intel/15/composer_xe_2015.2.164/mpirt/lib/mic:/opt/apps/intel/15/composer_xe_2015.2.164/compiler/lib/mic" MPISPAWN_GENERIC_NAME_99=_ MPISPAWN_GENERIC_VALUE_99="/usr/local/bin/build_env.pl" MPISPAWN_GENERIC_NAME_100=__BASHRC_SOURCED__ MPISPAWN_GENERIC_VALUE_100="1" MPISPAWN_GENERIC_NAME_101=ARMCI_OPENIB_DEVICE MPISPAWN_GENERIC_VALUE_101="mlx4_0" MPISPAWN_GENERIC_NAME_102=ENVIRONMENT MPISPAWN_GENERIC_VALUE_102="BATCH" MPISPAWN_GENERIC_NAME_103=_ModuleTable003_ MPISPAWN_GENERIC_VALUE_103="bGVmaWxlcyIsIi9vcHQvYXBwcy9pbnRlbDE1L21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL3hzZWRlL21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL21vZHVsZWZpbGVzIiwiL29wdC9tb2R1bGVmaWxlcyIsfSxbInN5c3RlbUJhc2VNUEFUSCJdPSIvb3B0L2FwcHMveHNlZGUvbW9kdWxlZmlsZXM6L29wdC9hcHBzL21vZHVsZWZpbGVzOi9vcHQvbW9kdWxlZmlsZXMiLH0=" MPISPAWN_GENERIC_NAME_104=SLURM_TACC_NODES MPISPAWN_GENERIC_VALUE_104="16" MPISPAWN_GENERIC_NAME_105=MV2_IBA_HCA MPISPAWN_GENERIC_VALUE_105="mlx4_0" MPISPAWN_GENERIC_NAME_106=SINK_LD_LIBRARY_PATH MPISPAWN_GENERIC_VALUE_106="/opt/apps/intel/15/composer_xe_2015.2.164/compiler/lib/mic:/opt/apps/intel/15/composer_xe_2015.2.164/mkl/lib/mic" MPISPAWN_GENERIC_NAME_107=QTINC MPISPAWN_GENERIC_VALUE_107="/usr/lib64/qt-3.3/include" MPISPAWN_GENERIC_NAME_108=CVS_RSH MPISPAWN_GENERIC_VALUE_108="ssh" MPISPAWN_GENERIC_NAME_109=ICC_BIN MPISPAWN_GENERIC_VALUE_109="/opt/apps/intel/15/composer_xe_2015.2.164/bin/intel64" MPISPAWN_GENERIC_NAME_110=USER MPISPAWN_GENERIC_VALUE_110="tg839601" MPISPAWN_GENERIC_NAME_111=MIC_I_MPI_DAPL_PROVIDER MPISPAWN_GENERIC_VALUE_111="ofa-v2-mlx4_0-1u" MPISPAWN_GENERIC_NAME_112=SLURM_JOB_QOS MPISPAWN_GENERIC_VALUE_112="normal" MPISPAWN_GENERIC_NAME_113=G_BROKEN_FILENAMES MPISPAWN_GENERIC_VALUE_113="1" MPISPAWN_GENERIC_NAME_114=SLURM_TACC_CORES MPISPAWN_GENERIC_VALUE_114="16" MPISPAWN_GENERIC_NAME_115=LMOD_DIR MPISPAWN_GENERIC_VALUE_115="/opt/apps/lmod/lmod/libexec" MPISPAWN_GENERIC_NAME_116=OMP_NUM_THREADS MPISPAWN_GENERIC_VALUE_116="1" MPISPAWN_GENERIC_NAME_117=__PERSONAL_PATH__ MPISPAWN_GENERIC_VALUE_117="1" MPISPAWN_GENERIC_NAME_118=SLURM_JOB_NAME MPISPAWN_GENERIC_VALUE_118="auto-particle-mpi16" MPISPAWN_GENERIC_NAME_119=SLURM_SUBMIT_HOST MPISPAWN_GENERIC_VALUE_119="login3.stampede.tacc.utexas.edu" MPISPAWN_GENERIC_NAME_120=SLURM_QUEUE MPISPAWN_GENERIC_VALUE_120="development" MPISPAWN_GENERIC_NAME_121=SLURM_TACC_ACCOUNT MPISPAWN_GENERIC_VALUE_121="TG-CCR140008" MPISPAWN_GENERIC_NAME_122=TBBROOT MPISPAWN_GENERIC_VALUE_122="/opt/apps/intel/15/composer_xe_2015.2.164/tbb" MPISPAWN_GENERIC_NAME_123=MPICH_HOME MPISPAWN_GENERIC_VALUE_123="/opt/apps/intel15/mvapich2/2.1" MPISPAWN_GENERIC_NAME_124=SLURM_COMMAND MPISPAWN_GENERIC_VALUE_124="sbatch" MPISPAWN_GENERIC_NAME_125=I_MPI_DAPL_PROVIDER MPISPAWN_GENERIC_VALUE_125="ofa-v2-mlx4_0-1u" MPISPAWN_GENERIC_NAME_126=SLURM_JOB_NODELIST MPISPAWN_GENERIC_VALUE_126="c557-[903-904],c558-[001-004,101-104,201-204,301-302]" MPISPAWN_GENERIC_NAME_127=SLURM_JOB_ACCOUNT MPISPAWN_GENERIC_VALUE_127="TG-CCR140008" MPISPAWN_GENERIC_NAME_128=SLURM_NNODES MPISPAWN_GENERIC_VALUE_128="16" MPISPAWN_GENERIC_NAME_129=MKLROOT MPISPAWN_GENERIC_VALUE_129="/opt/apps/intel/15/composer_xe_2015.2.164/mkl" MPISPAWN_GENERIC_NAME_130=SLURM_TACC_NCORES_SET MPISPAWN_GENERIC_VALUE_130="1" MPISPAWN_GENERIC_NAME_131=__Init_Default_Modules MPISPAWN_GENERIC_VALUE_131="1" MPISPAWN_GENERIC_NAME_132=SLURM_TOPOLOGY_ADDR_PATTERN MPISPAWN_GENERIC_VALUE_132="node" MPISPAWN_GENERIC_NAME_133=BASH_ENV MPISPAWN_GENERIC_VALUE_133="/etc/tacc/tacc_functions" MPISPAWN_GENERIC_NAME_134=TACC_XALT_BIN MPISPAWN_GENERIC_VALUE_134="/opt/apps/xalt/0.6/bin" MPISPAWN_GENERIC_NAME_135=X509_USER_PROXY MPISPAWN_GENERIC_VALUE_135="/tmp/x509up_p34218.fileHy64Io.1" MPISPAWN_GENERIC_NAME_136=LMOD_FULL_SETTARG_SUPPORT MPISPAWN_GENERIC_VALUE_136="no" MPISPAWN_GENERIC_NAME_137=LMOD_VERSION MPISPAWN_GENERIC_VALUE_137="7.3.18" MPISPAWN_ID=1 MPISPAWN_WORKING_DIR=/work/04661/tg839601/parallelhw2 MPISPAWN_MPIRUN_RANK_0=1 /opt/apps/intel15/mvapich2/2.1/bin/mpispawn 0
[c557-903.stampede.tacc.utexas.edu:mpispawn_0][report_error] connect() failed: Connection refused (111)
Past MPI Bcast
Past MPI Bcast
Past MPI Bcast
Past MPI Bcast
Past MPI Bcast
Past MPI Bcast
Past MPI Bcast
Past MPI Bcast
Past MPI Bcast
Past MPI Bcast
Past MPI Bcast
Past MPI Bcast
Past MPI Bcast
Past MPI Bcast
Past MPI Bcast
Past MPI Bcast
Past MPI Barrier and about to compute forces
Finished computing forces
About to move particlesPast MPI Barrier and about to compute forces
Past MPI Barrier and about to compute forces
Finished computing forces
About to move particlesPast MPI Barrier and about to compute forces
Finished computing forces
About to move particlesPast MPI Barrier and about to compute forces
Past MPI Barrier and about to compute forces
Finished computing forces
About to move particlesPast MPI Barrier and about to compute forces
Finished computing forces
About to move particlesPast MPI Barrier and about to compute forces
Finished computing forces
About to move particlesPast MPI Barrier and about to compute forces
Finished computing forces
About to move particlesPast MPI Barrier and about to compute forces
Finished computing forces
About to move particlesPast MPI Barrier and about to compute forces
Finished computing forces
About to move particlesFinished computing forces
About to move particlesPast MPI Barrier and about to compute forces
Finished computing forces
About to move particlesPast MPI Barrier and about to compute forces
Finished computing forces
About to move particlesPast MPI Barrier and about to compute forces
Finished computing forces
About to move particlesPast MPI Barrier and about to compute forces
Finished computing forces
About to move particlesPast MPI Barrier and about to compute forces
Finished computing forces
About to move particlesFinished computing forces
About to move particlesMoved particles
ErrorErrorErrorErrorErrorErrorErrorErrorErrorMoved particles
[c558-201.stampede.tacc.utexas.edu:mpi_rank_10][error_sighandler] Caught error: Segmentation fault (signal 11)
[c557-903.stampede.tacc.utexas.edu:mpispawn_0][readline] Unexpected End-Of-File on file descriptor 5. MPI process died?
[c557-903.stampede.tacc.utexas.edu:mpispawn_0][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c558-002.stampede.tacc.utexas.edu:mpispawn_3][readline] Unexpected End-Of-File on file descriptor 5. MPI process died?
[c558-002.stampede.tacc.utexas.edu:mpispawn_3][mtpmi_processops] Error while reading PMI socket. MPI process died?
Moved particles
Moved particles
[c558-204.stampede.tacc.utexas.edu:mpi_rank_13][error_sighandler] Caught error: Segmentation fault (signal 11)
Moved particles
[c558-001.stampede.tacc.utexas.edu:mpispawn_2][readline] Unexpected End-Of-File on file descriptor 5. MPI process died?
[c558-001.stampede.tacc.utexas.edu:mpispawn_2][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c558-201.stampede.tacc.utexas.edu:mpispawn_10][readline] Unexpected End-Of-File on file descriptor 5. MPI process died?
[c558-201.stampede.tacc.utexas.edu:mpispawn_10][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c558-301.stampede.tacc.utexas.edu:mpispawn_14][readline] Unexpected End-Of-File on file descriptor 5. MPI process died?
[c558-301.stampede.tacc.utexas.edu:mpispawn_14][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c558-204.stampede.tacc.utexas.edu:mpispawn_13][readline] Unexpected End-Of-File on file descriptor 5. MPI process died?
[c558-204.stampede.tacc.utexas.edu:mpispawn_13][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c558-002.stampede.tacc.utexas.edu:mpispawn_3][child_handler] MPI process (rank: 3, pid: 123272) exited with status 3
[c557-903.stampede.tacc.utexas.edu:mpispawn_0][child_handler] MPI process (rank: 0, pid: 17783) exited with status 3
[c558-001.stampede.tacc.utexas.edu:mpispawn_2][child_handler] MPI process (rank: 2, pid: 116035) exited with status 3
[c558-201.stampede.tacc.utexas.edu:mpispawn_10][child_handler] MPI process (rank: 10, pid: 48376) terminated with signal 11 -> abort job
[c558-301.stampede.tacc.utexas.edu:mpispawn_14][child_handler] MPI process (rank: 14, pid: 81510) exited with status 3
[c558-204.stampede.tacc.utexas.edu:mpispawn_13][child_handler] MPI process (rank: 13, pid: 107171) terminated with signal 11 -> abort job
[c558-103.stampede.tacc.utexas.edu:mpispawn_8][readline] Unexpected End-Of-File on file descriptor 5. MPI process died?
[c558-103.stampede.tacc.utexas.edu:mpispawn_8][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c558-103.stampede.tacc.utexas.edu:mpispawn_8][child_handler] MPI process (rank: 8, pid: 7561) exited with status 3
[c558-104.stampede.tacc.utexas.edu:mpispawn_9][readline] Unexpected End-Of-File on file descriptor 5. MPI process died?
[c558-104.stampede.tacc.utexas.edu:mpispawn_9][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c558-203.stampede.tacc.utexas.edu:mpispawn_12][readline] Unexpected End-Of-File on file descriptor 5. MPI process died?
[c558-203.stampede.tacc.utexas.edu:mpispawn_12][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c558-104.stampede.tacc.utexas.edu:mpispawn_9][child_handler] MPI process (rank: 9, pid: 48321) exited with status 3
[c558-203.stampede.tacc.utexas.edu:mpispawn_12][child_handler] MPI process (rank: 12, pid: 88158) exited with status 3
[c557-904.stampede.tacc.utexas.edu:mpispawn_1][readline] Unexpected End-Of-File on file descriptor 5. MPI process died?
[c557-904.stampede.tacc.utexas.edu:mpispawn_1][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c557-904.stampede.tacc.utexas.edu:mpispawn_1][child_handler] MPI process (rank: 1, pid: 7937) exited with status 3
[c558-102.stampede.tacc.utexas.edu:mpispawn_7][readline] Unexpected End-Of-File on file descriptor 5. MPI process died?
[c558-102.stampede.tacc.utexas.edu:mpispawn_7][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c558-102.stampede.tacc.utexas.edu:mpispawn_7][child_handler] MPI process (rank: 7, pid: 89638) exited with status 3
TACC: MPI job exited with code: 1
 
TACC: Shutdown complete. Exiting.
TACC: Starting up job 8363786
TACC: Setting up parallel environment for MVAPICH2+mpispawn.
TACC: Starting parallel tasks...
[c557-903.stampede.tacc.utexas.edu:mpispawn_0][report_error] connect() failed: Connection refused (111)
[c558-003.stampede.tacc.utexas.edu:mpispawn_4][read_size] Unexpected End-Of-File on file descriptor 6. MPI process died?
[c558-003.stampede.tacc.utexas.edu:mpispawn_4][read_size] Unexpected End-Of-File on file descriptor 6. MPI process died?
[c558-003.stampede.tacc.utexas.edu:mpispawn_4][handle_mt_peer] Error while reading PMI socket. MPI process died?
[c558-101.stampede.tacc.utexas.edu:mpispawn_6][read_size] Unexpected End-Of-File on file descriptor 6. MPI process died?
[c558-101.stampede.tacc.utexas.edu:mpispawn_6][read_size] Unexpected End-Of-File on file descriptor 6. MPI process died?
[c558-101.stampede.tacc.utexas.edu:mpispawn_6][handle_mt_peer] Error while reading PMI socket. MPI process died?
[c558-004.stampede.tacc.utexas.edu:mpispawn_5][read_size] Unexpected End-Of-File on file descriptor 6. MPI process died?
[c558-004.stampede.tacc.utexas.edu:mpispawn_5][read_size] Unexpected End-Of-File on file descriptor 6. MPI process died?
[c558-004.stampede.tacc.utexas.edu:mpispawn_5][handle_mt_peer] Error while reading PMI socket. MPI process died?
[c558-202.stampede.tacc.utexas.edu:mpispawn_11][read_size] Unexpected End-Of-File on file descriptor 6. MPI process died?
[c558-202.stampede.tacc.utexas.edu:mpispawn_11][read_size] Unexpected End-Of-File on file descriptor 6. MPI process died?
[c558-202.stampede.tacc.utexas.edu:mpispawn_11][handle_mt_peer] Error while reading PMI socket. MPI process died?
[c558-302.stampede.tacc.utexas.edu:mpispawn_15][read_size] Unexpected End-Of-File on file descriptor 6. MPI process died?
[c558-302.stampede.tacc.utexas.edu:mpispawn_15][read_size] Unexpected End-Of-File on file descriptor 6. MPI process died?
[c558-302.stampede.tacc.utexas.edu:mpispawn_15][handle_mt_peer] Error while reading PMI socket. MPI process died?
[c558-002.stampede.tacc.utexas.edu:mpispawn_3][report_error] connect() failed: Connection refused (111)
[c558-001.stampede.tacc.utexas.edu:mpispawn_2][report_error] connect() failed: Connection refused (111)
[c558-201.stampede.tacc.utexas.edu:mpispawn_10][report_error] connect() failed: Connection refused (111)
[c558-301.stampede.tacc.utexas.edu:mpispawn_14][report_error] connect() failed: Connection refused (111)
[c558-204.stampede.tacc.utexas.edu:mpispawn_13][report_error] connect() failed: Connection refused (111)
[c558-103.stampede.tacc.utexas.edu:mpispawn_8][report_error] connect() failed: Connection refused (111)
[c558-104.stampede.tacc.utexas.edu:mpispawn_9][report_error] connect() failed: Connection refused (111)
[c558-203.stampede.tacc.utexas.edu:mpispawn_12][report_error] connect() failed: Connection refused (111)
[c557-904.stampede.tacc.utexas.edu:mpispawn_1][report_error] connect() failed: Connection refused (111)
[c558-102.stampede.tacc.utexas.edu:mpispawn_7][report_error] connect() failed: Connection refused (111)

Strong scaling estimates are :
 (speedup)
 (efficiency)    for
 threads/processors

Average strong scaling efficiency:    -nan 

Weak scaling estimates are :
 (efficiency)    for
 threads/processors

Average weak scaling efficiency:    -nan 


mpi Grade =    -nan

 
TACC: Shutdown complete. Exiting.
